# Vision Transformer on CIFAR-10
# Example configuration for training a small ViT model

model:
  name: vit
  img_size: [32, 32]
  patch_size: 8
  in_channels: 3
  embed_dim: 256
  num_heads: 8
  num_layers: 4
  mlp_ratio: 2.0
  dropout: 0.1
  num_classes: 10
  positional_embedding: learned-1d
  use_cls_token: true

optimizer:
  name: adamw
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.999]

scheduler:
  name: cosine_warmup
  warmup_epochs: 3

data:
  dataset: cifar10
  data_dir: ../data
  batch_size: 128
  num_workers: 4
  pin_memory: true
  random_horizontal_flip: true
  random_crop: true
  crop_scale: [0.8, 1.0]
  crop_ratio: [0.9, 1.1]

num_epochs: 25
device: cuda:0
seed: 42
checkpoint_dir: ../data/checkpoints
save_best: true
log_every: 50
experiment_name: vit_cifar10_baseline
