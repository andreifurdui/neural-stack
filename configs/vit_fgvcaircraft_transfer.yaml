# Vision Transformer on CIFAR-10
# Example configuration for training a small ViT model

model:
  name: vit_small_patch32_224.augreg_in21k
  source: timm
  pretrained: true
  img_size: [224, 224]
  num_classes: 100

optimizer:
  name: adamw
  lr: 0.003
  weight_decay: 0.03
  betas: [0.9, 0.999]

scheduler:
  name: cosine_warmup
  warmup_epochs: 5

data:
  dataset: fgvc-aircraft
  data_dir: ../data
  batch_size: 128
  num_workers: 6
  pin_memory: true
  random_horizontal_flip: true
  random_crop: true
  crop_scale: [0.8, 1.0]
  crop_ratio: [0.9, 1.1]
  color_jitter: true
  jit_brightness: 0.1
  jit_contrast: 0.1
  jit_saturation: 0.1
  jit_hue: 0.1
  rand_aug: true
  rand_aug_num_ops: 2
  rand_aug_magnitude: 6

grad_clip_norm: 1.0
use_amp: true

num_epochs: 50
device: cuda:0
seed: 42
checkpoint_dir: ../data/checkpoints
save_best: false
log_every: 10
experiment_name: vit_cifar10_baseline
